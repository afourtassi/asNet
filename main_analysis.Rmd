---
title: "Comparison of TD, AS"
author: "_"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(stringr)
library(boot)
library(purrr)
library(ggplot2)
library(ggthemes)
library(feather)
library(poweRlaw)
library(wordbankr)

#Load helper functions
source(paste(getwd(),"/helper_functions/all_helper.r",sep = ""), chdir = T)

#http://macappstore.org/espeak/

#Load probabilsitic models
source(paste(getwd(),"/models/all_models.r",sep = ""), chdir = T)

```

Load data_TD, get data_median_TD
```{r}
languages = "English (American)"

admins <- get_administration_data() %>%
  select(data_id, age, language, form, production, comprehension) %>%
  filter(language %in% languages)

lang_ns <- admins %>% 
  group_by(language, form) %>% 
  summarise(n_admins = n())

items <- get_item_data() %>%
  filter(type == "word") %>%
  filter(language %in% languages)

num_words <- items %>%
  group_by(language, form) %>%
  summarise(n = n())


vocab_data <- admins %>%
  select(data_id, language, form, age, production, comprehension) %>% 
  left_join(num_words) %>%
  mutate(no_production = n - production)

data_median_TD <- vocab_data %>%
  group_by(form, language, age) %>%
  summarise(production = median(production), 
            production_prop = median(production/n), 
            n = n()) %>%
  left_join(lang_ns) %>%
  filter(form == "WS")
```


```{r}
items_by_inst <- split(items, paste(items$language, items$form, sep = "_"))

get_inst_data <- function(inst_items) {
  inst_lang <- unique(inst_items$language)
  inst_form <- unique(inst_items$form)
  inst_admins <- filter(admins, language == inst_lang, form == inst_form)
  get_instrument_data(language = inst_lang ,
                      form = inst_form,
                      administrations = inst_admins,
                      items = inst_items$item_id,
                      iteminfo = inst_items) %>%
    filter(!is.na(age)) %>%
    mutate(produces = !is.na(value) & value == "produces",
           understands = !is.na(value) & (value == "understands" | value == "produces")) %>%
    select(-value) %>%
    gather(measure, value, produces, understands) %>% # produces and understands are values 
    filter((measure == "understands" & form == "WG") | (measure == "produces" & form == "WS") ) %>%
    mutate(language = inst_lang,
           form = inst_form)
    
}

data_raw_TD <- map(items_by_inst, get_inst_data) 
  
data_raw_TD <- bind_rows(data_raw_TD) %>%
  rename(item = num_item_id) %>%
  group_by(language, form, measure,
             lexical_category, lexical_class, uni_lemma,  item, definition, age) %>%
  summarise(num_true = sum(value, na.rm = TRUE),
              num_false = n() - num_true,
              prop = mean(value, na.rm = TRUE)) 

#feather::write_feather(data_raw_TD,"saved_data/data_raw_TD.feather")
#data_raw_TD <- feather::read_feather("saved_data/data_raw_TD.feather")%>%
  #filter(form == "WS")
```


Load data_AS
```{r}
data_AS <- read.delim("mci_sentences02.txt", header = TRUE, sep = "\t", dec = ".")

# extracting first row as a descriptive dataframe
description <- data_AS[1:1,]
description <- as.data.frame(t(description))
names(description) <- "description"

#using mci_sentences02_id	as a disctincetive id and The NDAR Global Unique Identifier 

data_raw_AS <- data_AS %>%
  select(c("mci_sentences02_id","subjectkey","interview_age", 21:779)) %>% # starting from 785 is complexity
  select(-(684:692)) #getting rid of the long questions in the middle

colnames(data_raw_AS) <- as.character(unlist(data_raw_AS[1,])) #unlist the row
data_raw_AS = data_raw_AS[-1, ]

data_raw_AS<- data_raw_AS %>%
  rename(id = "mci_sentences02_id",
         GUID = "The NDAR Global Unique Identifier (GUID) for research subject", 
         age = "Age in months at the time of the interview/test/sampling/imaging.")%>%
  mutate(age = as.numeric(as.character(age)))

data_clean_AS <- data_raw_AS %>%
  gather(key = "definition", value = "value", -c(id,GUID,age))%>%
  separate(definition, c("category","definition"),sep = "\\. ") %>%
  mutate_all(na_if,"",)%>% #if blank then fill in NA
   mutate(value_1 = ifelse(value == 0, FALSE, TRUE),
          value_2 = ifelse(value == 1|value == 3, TRUE, FALSE))

data_all_AS <- data_clean_AS %>%
     group_by(category, definition, age) %>%
       summarise(num_true_1 = sum(value_1, na.rm = TRUE),
                 num_false_1 = n() - num_true_1,
                 prop_1 = num_true_1/n(), 
                 num_true_2 = sum(value_2, na.rm = TRUE),
                 num_false_2 = n() - num_true_2,
                 prop_2 = num_true_2/n())

summary(data_all_AS)
```

Compute Age of Acqusition (AoA) of definitions

#https://github.com/langcog/wordbank-book/blob/master/104-appendix-aoa.Rmd
baysian GLM

```{r}
fit_inst_measure_uni <- function(inst_measure_uni_data) {
  ages <- min(inst_measure_uni_data$age):max(inst_measure_uni_data$age)
  model <- glm(cbind(num_true, num_false) ~ age, family = "binomial",
               data = inst_measure_uni_data)
  fit <- predict(model, newdata = data.frame(age = ages), se.fit = TRUE)
  aoa <- -model$coefficients[["(Intercept)"]] / model$coefficients[["age"]]
  constants <- inst_measure_uni_data %>%
    ungroup()%>%
    select(category, definition) %>%
    distinct()
  
  props <- inst_measure_uni_data %>%
    ungroup() %>%
    select(age, prop)
  
  data.frame(age = ages,
             fit_prop = inv.logit(fit$fit),
             fit_se = fit$se.fit,
             aoa = aoa, 
             category = constants$category,
             definition = constants$definition) %>%
    left_join(props)
}

list_by_item_1 <- data_all_AS %>% 
  rename(num_true = num_true_1,
         num_false = num_false_1,
         prop = prop_1)%>%
  split(paste(.$category,.$definition)) # need comfirmation 

data_aoa_1 <- map(list_by_item_1, fit_inst_measure_uni) %>% 
  bind_rows()

data_aoa_1 <- data_aoa_1 %>%
  select(category,definition,aoa) %>%
  distinct()  # negative values in aoa, same in main_analysis.rmd

list_by_item_2 <- data_all_AS %>% 
  rename(num_true = num_true_2,
         num_false = num_false_2,
         prop = prop_2)%>%
  split(paste(.$category,.$definition)) # need comfirmation 

data_aoa_2 <- map(list_by_item_2, fit_inst_measure_uni) %>% 
  bind_rows()

data_aoa_2 <- data_aoa_2 %>%
  select(category,definition,aoa) %>%
  distinct()

data_aoa_AS <- inner_join(data_aoa_1,data_aoa_2,by = c("category","definition"))%>%
  rename(aoa_1 = aoa.x, aoa_2 = aoa.y)
#feather::write_feather(data_aoa_AS, "saved_data/data_aoa_AS.feather")
data_aoa_AS <- feather::read_feather("saved_data/data_aoa_AS.feather") 
```

```{r}
data_aoa_TD <- feather::read_feather("saved_data/data_aoa_TD_2.feather")%>%
  filter(measure == "produces")

all_aoa <- inner_join(data_aoa_TD,data_aoa_AS, by = c("uni_lemma" = "definition"))%>%
  rename(aoa_TD = aoa,
         aoa_AS_1 = aoa_1,
         aoa_AS_2 = aoa_2)%>%
  filter(aoa_TD > 0)%>% # note that i've applied filter here 
  mutate(diff_1 = aoa_AS_1 - aoa_TD,
         diff_2 = aoa_AS_2 - aoa_TD,
         diff_2_perc = (aoa_AS_2 - aoa_TD)/aoa_TD*100)%>%
  arrange(desc(diff_2_perc)) 
  
most_diff_aoa <-  all_aoa %>% head(10)
least_diff_aoa <- tail(all_aoa, 10)

diff_aoa <- bind_rows(least_diff_aoa,most_diff_aoa)

ggplot(diff_aoa, aes(x = lexical_class, y = diff_2_perc))+
  geom_point(alpha = 0.8)+
  labs(y = "Percentage Change in AoA", x = "Lexical Class", title = "Top & Bottom 10 in Percentage difference in AoA")+
  theme_bw()
  

ggplot(all_aoa, aes(aoa_TD))+
  stat_density(geom="line")+
  geom_density(aes(aoa_AS_1), color = "blue")+
  geom_density(aes(aoa_AS_2),color = "red")+
  theme_classic()+
  labs(x = "AoA", title = "Density plot of Age of Acqusition", caption = "Black: TD, Blue = AS_method_1, Red = AS_method_2")+
  #facet_wrap(vars(lexical_class))

```

Median Produced comparison 

```{r}
data_median_1 <- data_clean_AS %>%
  filter(value_1 == TRUE)%>%
  group_by(GUID,age)%>%
  summarise(n = n_distinct(definition)) %>%
  group_by(age) %>%
  summarise(value = median(n))

data_median_2 <- data_clean_AS %>%
  filter(value_2 == TRUE)%>%
  group_by(GUID,age)%>%
  summarise(n = n_distinct(definition)) %>%
  group_by(age) %>%
  summarise(value = median(n))

data_median_AS <- inner_join(data_median_1,data_median_2,by = "age")%>%
  rename(value_1 = value.x, value_2 = value.y)

data_median_AS %>%
  filter(age <= 30, age >= 16)%>%
  ggplot(aes(x = age, y = value_1))+
  geom_point(col = "dodgerblue")+
  labs(y = "Median Word Production", x = "Age(Month)",title = "Median Word Production Comparison, method 1", caption = "Orange: TD, Blue: AS")+
  geom_point(data = data_median_TD, aes(x = age, y = production), color = "darkorange")+
  theme_classic()

data_median_AS %>%
  filter(age <= 30, age >= 16)%>%
  ggplot(aes(x = age, y = value_2))+
  #geom_point(col = "dodgerblue")+
  geom_smooth()+
  labs(y = "Median Word Production", x = "Age(Month)",title = "Median Word Production Comparison, method 2", caption = "Orange: TD, Blue: AS")+
  geom_smooth(data = data_median_TD, aes(x = age, y = production), color = "darkorange")+
  theme_classic()


# the methods barely influence the relationship in graph
```

Percentile graph 

```{r}
taus <-  c(0.1, 0.25, 0.5, 0.75, 0.9)

data_children_1 <- data_clean_AS %>%
  filter(value_1 == TRUE) %>%
  group_by(GUID,age)%>%
  summarise(n = n_distinct(definition))

data_children_2 <- data_clean_AS %>%
  filter(value_2 == TRUE) %>%
  group_by(GUID,age)%>%
  summarise(n = n_distinct(definition))

ggplot(data_children_1,
        aes(x  = age, y = n)) +
  geom_jitter(width = .4, size = 1, alpha = .4) +
  labs(y = "Production (number of words)", title = "Production vs. Age in AS children, method 1") +
  ylim(c(0, 680)) + 
  theme(legend.position = "bottom")+
  geom_quantile(quantiles = taus)

ggplot(data_children_2,
        aes(x  = age, y = n)) +
  geom_jitter(width = .4, size = 1, alpha = .4) +
  labs("Production (number of words)", title = "Production vs. Age in AS children, method 2") +
  ylim(c(0, 680)) + 
  theme(legend.position = "bottom")+
  geom_quantile(quantiles = taus)
```


exploratory data analysis on AS children 

```{r}
count <- data_raw_AS %>%
  group_by(age)%>%
  summarise(N = n())

ggplot(count, aes(x = age, y = N))+
  geom_line()+
  labs(y = "Number of Children on Record", title = "Number of Children with Autism on Record")

count_a15 <- count %>% filter(N > 15) # count above 15 

data_median_n <- left_join(count_a15, data_clean_AS, by = "age") %>%
  filter(value_2 == TRUE)%>%
  group_by(GUID,age)%>%
  summarise(n = n_distinct(definition)) %>%
  group_by(age) %>%
  summarise(value = median(n))

data_median_n %>%
  filter(age <= 30, age >= 16)%>%
  ggplot(aes(x = age, y = value))+
  geom_point(col = "dodgerblue")+
  geom_smooth()+
  labs(y = "Median Word Production", x = "Age(Month)",title = "Median Word Production Comparison", caption = "Orange: TD, Blue: AS. Without data point that have less than 15 obersvations")+
  geom_smooth(data = data_median_TD, aes(x = age, y = production), color = "darkorange")+
  theme_classic()

data_children_n <- left_join(count_a15, data_clean_AS, by = "age")%>%
  filter(value_2 == TRUE) %>%
  group_by(GUID,age)%>%
  summarise(n = n_distinct(definition))

ggplot(data_children_n,
        aes(x  = age, y = n)) +
  geom_jitter(width = .4, size = 1, alpha = .4) +
  labs("Production (number of words)", title = "Production vs. Age in AS children",caption = "Without data point that have less than 15 obersvations") +
  ylim(c(0, 680)) + 
  theme(legend.position = "bottom")+
  geom_quantile(quantiles = taus)


```


Two word comparison

```{r}
ball_AS <- data_all_AS%>%
  filter(definition == "ball", age <= 60)

ball_AS_clean <- left_join(count_a15, ball_AS, by = "age")

ball_TD <-data_raw_TD %>%
  filter(definition == "ball",
         form == "WS")

mod_1<- glm(prop ~ age, family = "binomial",
               data = ball_TD)

fitted_points_ball_TD <- mod_1 %>%
  broom::augment()%>%
  mutate(fitted_prob = 1/(1 + exp(-.fitted)))

ggplot(ball_AS, aes(x = age, y = prop_1))+
  geom_point()+
  geom_smooth(se = FALSE,method = 'loess')+
  geom_point(data = ball_TD, aes(x =  age, y = prop), col = "red")+
  labs(y = "Prop", x = "Age (Month)", title = "Proportion of Children who could produce the noun: Ball, method 1") +
  geom_hline(yintercept=.5, linetype="dashed", color = "blue")+
  theme(legend.position = "bottom")+
  theme_classic()

ggplot(ball_AS, aes(x = age, y = prop_2))+
  geom_point()+
  geom_smooth(se = FALSE,method = 'loess')+
  geom_point(data = ball_TD, aes(x =  age, y = prop), col = "red")+
  labs(y = "Prop", x = "Age (Month)", title = "Proportion of Children who could produce the noun: Ball, method 2") +
  geom_hline(yintercept=.5, linetype="dashed", color = "blue")+
  theme(legend.position = "bottom")+
  theme_classic()


play_AS <- data_all_AS %>%
  filter(definition == "play",
         age <= 90)

play_AS_clean <- left_join(count_a15, play_AS, by = "age")

play_TD <- data_raw_TD %>%
  filter(definition == "play")%>%
  filter(form == "WS")

mod_2<- glm(prop ~ age, family = "binomial",
               data = play_TD)

fitted_points_play_TD <- mod_2 %>%
  broom::augment()%>%
  mutate(fitted_prob = 1/(1 + exp(-.fitted)))


ggplot(play_AS, aes(x = age, y = prop_1))+
  geom_point()+
  geom_smooth(se = FALSE,method = 'loess')+
  geom_point(data = play_TD, aes(x =  age, y = prop), col = "red")+
  labs(y = "Prop", x = "Age (Month)", title = "Proportion of Children who could produce the verb: play, method 1") +
  scale_x_continuous(breaks =c(0,10,20,30,40,50,60,70,80))+
  geom_hline(yintercept=.5, linetype="dashed", color = "blue")+
  theme(legend.position = "bottom")+
  theme_classic()

ggplot(play_AS, aes(x = age, y = prop_2))+
  geom_point()+
  geom_smooth(se = FALSE,method = 'loess')+
  geom_smooth(data = fitted_points_play_TD, aes(x =age, y = fitted_prob), col = "red")+
  geom_point(data = play_TD, aes(x =  age, y = prop), col = "red")+
  labs(y = "Prop", x = "Age (Month)", title = "Proportion of Children who could produce the verb: play, method 2") +
  theme(legend.position = "bottom")+
  scale_x_continuous(breaks =c(0,10,20,30,40,50,60,70,80))+
  geom_hline(yintercept=.5, linetype="dashed", color = "blue")+
  theme_classic()

ggplot(ball_AS_clean, aes(x = age, y = prop_2))+
  geom_point()+
  geom_smooth(method = 'loess')+
    geom_smooth(data = fitted_points_ball_TD, aes(x =age, y = fitted_prob), col = "red")+
  geom_point(data = ball_TD, aes(x =  age, y = prop), col = "red")+
  labs(y = "Prop", x = "Age (Month)", title = "Proportion of Children who could produce the noun: Ball, method 2", caption = "Without data point that have less than 15 obersvations") +
  geom_hline(yintercept=.5, linetype="dashed", color = "blue")+
  theme(legend.position = "bottom")+
  theme_classic()

ggplot(play_AS_clean, aes(x = age, y = prop_2))+
  geom_point()+
  geom_smooth(methosd = 'loess')+
  geom_smooth(data = fitted_points_play_TD, aes(x =age, y = fitted_prob), col = "red")+
  geom_point(data = play_TD, aes(x =  age, y = prop), col = "red")+
  labs(y = "Prop", x = "Age (Month)", title = "Proportion of Children who could produce the verb: play, method 2", caption = "Without data point that have less than 15 obersvations") +
  theme(legend.position = "bottom")+
  scale_x_continuous(breaks =c(0,10,20,30,40,50,60,70,80))+
  geom_hline(yintercept=.5, linetype="dashed", color = "blue")+
  theme_classic()

```

make data growth dataframe

```{r}
make_data_growth <- function(word_aoa) {
  
  measure <- unique(word_aoa$measure)
  lang <- unique(word_aoa$language)
    
  ages<- (word_aoa %>%
    arrange(age) %>%
    group_by(age) %>%
    summarise(n = n()))$age
  
  df <- data.frame()
  for (i in ages) {
    rem_words <- word_aoa %>% filter(age >= i)
    rem_lemma <- c(rem_words$uni_lemma)
    rem_def <- c(rem_words$definition)
    rem_item<- c(rem_words$item)
    corr_age <- rep(i, times = length(rem_lemma))
    curr_df <- data.frame(corr_age, rem_item, rem_lemma, rem_def)
    df <- rbind(df, curr_df)
  }  
  df <- df %>% rename(uni_lemma = rem_lemma, definition=rem_def, item=rem_item)%>%
    left_join(word_aoa %>% select(item, age)) %>%
    mutate(learned = as.numeric(age == corr_age)) %>%
    select(corr_age, item, definition, uni_lemma,learned) %>%
    rename(age = corr_age) %>%
    arrange(age, item) %>%
    mutate(language = lang,
           measure = measure)
  return(df)
}

aoa_by_measure_lang <- all_aoa %>%
  mutate(age = round(aoa_AS_2)) %>%
  select(-form, -aoa_AS_2) %>%
  filter(lexical_class == "nouns")%>%
  split(paste(.$language, .$measure))
  
  
data_growth <- map(aoa_by_measure_lang, make_data_growth) %>%
  bind_rows()%>%
  filter(age <= 45)# start with 24 since AoA for Children with AS start with 24.# used common_word dataset.Limit to age < 45 (more than 15 observations)

#feather::write_feather(data_growth,"saved_data/data_growth_AS.feather") # take out data_growth(TD) when done 

data_growth <- feather::read_feather("saved_data/data_growth_AS.feather") 
```




Construct the netwroks and derive the dynamic predictors: PAC and PAT
```{r}
words_growth <- data_growth

growth_by_meas <- words_growth %>%
  split(paste(.$measure))

growth_by_meas_lang <- words_growth %>%
  split(paste(.$measure, .$language))

#Semantic netwrok 
##################

sem_net_fun  <- function (growth_meas_lang){
  
  first_age<- growth_meas_lang$age[1]
  
  lemma_list<- growth_meas_lang %>%
      trim_all_unilemma() %>% #For semantic netowtks only the unilmma
      filter(age==first_age) %>%
      select(item, uni_lemma)

assoc_pairs<- make_assoc_pairs(lemma_list = lemma_list)

assoc_PAC<- PAC_generator(vocab_age = growth_meas_lang, word_pairs = assoc_pairs) %>% 
      rename(PAC_assoc = value) %>% select(-definition, -uni_lemma)

 assoc_PAT<- PAT_generator(vocab_age = growth_meas_lang, word_pairs = assoc_pairs) %>% 
      rename(PAT_assoc = value) %>% select(-definition, -uni_lemma)
 
 sem_growth <- growth_meas_lang %>%
   left_join(assoc_PAC) %>%
   left_join(assoc_PAT)
 
 return(sem_growth)
 
}

sem_red <-  map(growth_by_meas_lang, sem_net_fun) %>%
  bind_rows()

#Save 
#feather::write_feather(sem_red, "saved_data/sem_red.feather")
sem_red_2 <- feather::read_feather("saved_data/sem_red.feather") 

#Phonological Networks
######################


phono_net_fun <- function (growth_meas_lang) {
  
  lang <- unique(growth_meas_lang$language)
  #meas <- unique(growth_meas_lang$measure)
  
  first_age<- growth_meas_lang$age[1]
  
  def_list<- growth_meas_lang %>%
      trim_all_definition() %>% 
      filter(age==first_age) %>%
      select(item, definition)
  
  phono_pairs<- make_IPA_pairs(def_list = def_list, lang = lang)
  
  #Threshold the phonetic distance (we take t=2, becuase t=1 is too sparse) 
  threshold <- 2

  phono_PAC <- PAC_generator(vocab_age = growth_meas_lang, word_pairs = phono_pairs %>% IPA_threshold(threshold)) %>% 
      rename(PAC_phono_t2 = value) %>% select(-definition, -uni_lemma)
  
  phono_PAT <- PAT_generator(vocab_age = growth_meas_lang, word_pairs = phono_pairs %>% IPA_threshold(threshold)) %>% 
      rename(PAT_phono_t2 = value) %>% select(-definition, -uni_lemma)
  
  phono_growth <- growth_meas_lang %>%
   left_join(phono_PAC) %>%
   left_join(phono_PAT)
  
}

phono_red <-  map(growth_by_meas_lang, phono_net_fun) %>%
  bind_rows() 


#feather::write_feather(phono_red, "saved_data/phono_red.feather")
phono_red_2 <- feather::read_feather("saved_data/phono_red.feather") 

#Combine data 
data_growth_net <- words_growth %>%
  left_join(sem_red) %>%
  left_join(phono_red)
```

```{r,fig.width=10, fig.height=5}
#Compute other static predictors (frequency and length) besides PAC

data_static <- data_growth_net %>%
  distinct(language,uni_lemma, definition, item, PAC_assoc, PAC_phono_t2) %>%
  rename(sem_deg = PAC_assoc, 
         phono_deg =  PAC_phono_t2)
  
#Word length
words_len <- data_static %>%
  trim_all_definition() %>%
  rowwise() %>%
  mutate(IPA=Speak(lang = language, word = definition)) %>%
  trim_IPA_completely() %>%
  mutate(length = str_count(IPA)) %>%
  select(-definition, -uni_lemma, -sem_deg, -phono_deg)

load("saved_data/uni_joined.RData")
frequency_mika <- uni_joined %>%
  filter(lexical_classes =='nouns') %>%
  distinct(language, uni_lemma, frequency) %>%
  mutate(lang_temp = ifelse(language == "French (Quebec)", "French (Quebecois)", language)) %>%
  select(-language) %>%
  rename(language = lang_temp)

words_freq <- data_static %>%
  left_join(frequency_mika) %>%
  select(-definition, -uni_lemma, -sem_deg, -phono_deg)

aoa_items <- all_aoa %>%
  select(language, measure, item, aoa_AS_2) # Here I should round the AoA?
  
#Combine predictors
data_static_pred  <- data_static %>%
  left_join(words_len) %>%
  left_join(words_freq) %>%
  left_join(aoa_items) #%>%
  #select(-definition, -uni_lemma)

#feather::write_feather(data_static_pred, "saved_data/data_static_pred.feather")
data_static_pred_2 <- feather::read_feather("saved_data/data_static_pred.feather") 

#Combine with full proportion-based data  (for the second regression which fit the entire production curve)
data_static_pred$language <- plyr::mapvalues(data_static_pred$language, 
                                 from = "English (American)", 
                                 to = "English")


#data_static_prop$language <- plyr::mapvalues(data_static_prop$language, 
#                                 from = "English (American)", 
#                                 to = "English")

#data_static_prop <- data_all %>%
#  left_join(data_static_pred) 

```

```{r}
Production

unilemmas <- data_static_pred %>%
  select(-IPA) %>%
  rename(aoa = aoa_AS_2)%>%
  filter(!is.na(uni_lemma)) %>% 
  filter(!is.na(sem_deg))  #only keep unilemmas that intersect with free association data

#problems need to check 
uni_scale <- unilemmas %>%
  group_by(measure, language) %>%
  mutate_at(c('sem_deg', 'phono_deg', 'length', 'frequency'), funs(as.numeric(Hmisc::impute(.)))) %>%
  mutate_at(c('sem_deg', 'phono_deg', 'length', 'frequency'), funs(as.numeric(scale(.))))

#feather::write_feather(uni_scale, "saved_data/data_static_scaled.feather")

data_long <- uni_scale %>%
  gather(predictor, value, sem_deg:frequency) %>%
  filter(predictor == "sem_deg" | predictor == "phono_deg") 

correlations <- data_long %>%
  group_by(measure, language, predictor) %>%
  summarise(cor = round(cor(aoa, value), 2))

plot_correlation_prod <- ggplot(data_long, aes(x=value, y=aoa))+
  facet_grid(predictor ~ language)+
  geom_jitter(size = 0.9,col = "lightblue")+
  geom_abline(slope = -1)+
  coord_cartesian(xlim=c(-1,5))+
  #scale_x_continuous(limits=c(-2,5))+
  scale_y_continuous(breaks =c(15,25,35,45,55))+
  geom_smooth(method = "lm", colour = "grey1", se=FALSE)+
  #scale_colour_solarized(name = "") +
  theme_few()+
  theme(aspect.ratio = 0.7, 
        plot.margin=grid::unit(c(0,0,0,0), "mm")
        )+
  geom_text(data=subset(correlations), aes(label=paste("r=", cor, sep="")), x=3.5, y=50, size=4, fontface = "bold")+
  xlab("degree z-score") +ylab("AoA")
  
plot_correlation_prod
```


### Degree distribution
Import the analysese from cogsci paper

```{r}

#Data for plot
degreeDist <- data.frame(matrix(ncol = 5, nrow = 0))
dist_names <- c("x", "y", "dimension", "language", "measure")
colnames(degreeDist) <- dist_names
#Parameters and test
degreeTest <- data.frame(matrix(ncol = 6, nrow = 0))
test_names <- c("xMin", "alpha", "pVal", "dimension","language", "measure")
colnames(degreeTest) <- test_names
powerLaw_fun <- function(data_growth_meas_lang, analysis){
  
  lang <- unique(data_growth_meas_lang$language)
  meas <- unique(data_growth_meas_lang$measure)
  
  data_meas_lang <- data_growth_meas_lang %>%
    select(measure, language, sem_deg, phono_deg) %>%
    dplyr::rename(Sem = sem_deg, Phono = phono_deg) %>%
    dplyr::filter (!is.na(Sem),!is.na(Phono)) 
  
  ##Semantic network
  
  #fit and derive parameters for power law
  semList <- data_meas_lang$Sem[data_meas_lang$Sem != 0]
  sem_pl = displ$new(semList)
  sem_est = estimate_xmin(sem_pl)
  sem_pl$setXmin(sem_est)
  sem_dist = plot(sem_pl) %>%
    mutate(dimension='Sem', language  =  lang, measure = meas)
  
  #bootstrap to get p-value
  sem_boot = bootstrap_p(sem_pl, no_of_sims=1000, threads=2)
  
  sem_test <- data.frame(as.numeric(sem_pl$xmin), as.numeric(sem_pl$pars), as.numeric(sem_boot$p), 'Sem', lang, meas)
  colnames(sem_test) <- test_names
  
  ##Phonological network
  
  #fit and derive parameters for power law
  phonoList <- data_meas_lang$Phono[data_meas_lang$Phono != 0]
  phono_pl = displ$new(phonoList)
  phono_est = estimate_xmin(phono_pl)
  phono_pl$setXmin(phono_est)
  phono_dist = plot(phono_pl) %>%
    mutate(dimension='Phono', language = lang, measure = meas)
  
  #bootstrap to get p-value
  phono_boot = bootstrap_p(phono_pl, no_of_sims=1000, threads=2)
  
  phono_test <- data.frame(as.numeric(phono_pl$xmin), as.numeric(phono_pl$pars), as.numeric(phono_boot$p), 'Phono', lang, meas)
  colnames(phono_test) <- test_names
  
  dist_meas_lang <- bind_rows(sem_dist, phono_dist)
  test_meas_lang <- bind_rows(sem_test, phono_test)
  
  #return(dist_meas_lang)
  
  if (analysis == 'distribution') {
    
    return(dist_meas_lang)
    
  } else if (analysis == 'test') {
    
    return(test_meas_lang)
    
  } else {
    
    print("Please specify the analysis type ('distribution' or 'test')")
    
  }
  
}



#Split by measure and language


data_by_meas_lang <- unilemmas %>%
  split(paste(.$measure, .$language))

degree_dist  <- map2(data_by_meas_lang, 'distribution', powerLaw_fun) %>%
  bind_rows()


degree_test  <- map2(data_by_meas_lang, 'test' , powerLaw_fun) %>%
  bind_rows()

#feather::write_feather(degree_dist, "saved_data/degree_dist.feather")
degree_dist <- feather::read_feather("saved_data/degree_dist.feather")


#feather::write_feather(degree_test, "saved_data/degree_test.feather")
degree_test <- feather::read_feather("saved_data/degree_test.feather")

#plot cumulative distributions
ggplot(data = degree_dist,  aes(x=x, y=y, col=dimension))+
  facet_grid(measure ~ language)+#, scales = "free") +
  geom_point(#aes(colour = lexical_class),
    #colour = solarized_palette(1),
    size = 0.5, alpha = 0.5) +
  scale_y_log10() + scale_x_log10() +
  theme(aspect.ratio = 1)
  #scale_x_continuous(limits=c(-2,5))+
  #geom_smooth(method = "lm", colour = "grey3", se=FALSE)+
  #scale_colour_solarized(name = "") 

```


